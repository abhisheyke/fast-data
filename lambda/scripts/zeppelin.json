{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"lineNumbers":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460415803367_-817185995","id":"20160412-010323_766764087","dateCreated":"Apr 12, 2016 1:03:23 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1138","text":"var fileName = \"/home/scala-academy/Data/social_media.csv\"\r\nvar file = sc.textFile(fileName)\r\n\r\ncase class SocialMediaMessage(name: String, age:Int, source: String, date: Int, message: String, category: String)\r\n\r\nval data = file\r\n            .map(l => l.split(\",\"))\r\n            .filter(_(0) != \"Name\")\r\n            .map(l => SocialMediaMessage(\r\n                l(0),\r\n                l(1).toInt,\r\n                l(2),\r\n                l(3).toInt,\r\n                l(4),\r\n                l(5)))\r\n                \r\ndata.collect().foreach(println)\r\n","dateUpdated":"Apr 14, 2016 12:30:05 AM","dateFinished":"Apr 14, 2016 12:30:07 AM","dateStarted":"Apr 14, 2016 12:30:05 AM","title":"1. Load Data","result":{"code":"SUCCESS","type":"TEXT","msg":"fileName: String = /home/scala-academy/Data/social_media.csv\nfile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[40] at textFile at <console>:39\ndefined class SocialMediaMessage\ndata: org.apache.spark.rdd.RDD[SocialMediaMessage] = MapPartitionsRDD[43] at map at <console>:47\nSocialMediaMessage(Bas,37,Twitter,20160414,Preparing for the IoT Tech Day 2016!,Technology)\nSocialMediaMessage(Test,12,Facebook,20140511,I really like my dog.,Pets)\nSocialMediaMessage(John,18,Pinterest,20150904,New Fifa is really awesome,Games )\nSocialMediaMessage(Maria,21,Twitter,20160118,Going shopping to get new sneakers!,Shoes)\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"lineNumbers":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460584607547_946419265","id":"20160413-235647_1258154134","dateCreated":"Apr 13, 2016 11:56:47 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1161","text":"// Zeppelin creates and injects sc (SparkContext) and sqlContext (HiveContext or SqlContext)\r\n// So you don't need create them manually (val sqlContext = new org.apache.spark.sql.SQLContext(sc))\r\n//import sqlContext.implicits._\r\n\r\nval df = data.toDF()\r\n\r\ndf.show()\r\ndf.printSchema()\r\ndf.select(\"name\").show()\r\n\r\ndf.registerTempTable(\"dataframe1\")\r\n\r\nsqlContext.tableNames().foreach(println)\r\n\r\nval all = sqlContext.sql(\"SELECT * FROM dataframe1\")\r\nall.collect().foreach(println)\r\n","dateUpdated":"Apr 14, 2016 12:30:12 AM","dateFinished":"Apr 14, 2016 12:30:16 AM","dateStarted":"Apr 14, 2016 12:30:12 AM","title":"2. Create Data Frame","result":{"code":"SUCCESS","type":"TEXT","msg":"df: org.apache.spark.sql.DataFrame = [name: string, age: int, source: string, date: int, message: string, category: string]\n+-----+---+---------+--------+--------------------+----------+\n| name|age|   source|    date|             message|  category|\n+-----+---+---------+--------+--------------------+----------+\n|  Bas| 37|  Twitter|20160414|Preparing for the...|Technology|\n| Test| 12| Facebook|20140511|I really like my ...|      Pets|\n| John| 18|Pinterest|20150904|New Fifa is reall...|    Games |\n|Maria| 21|  Twitter|20160118|Going shopping to...|     Shoes|\n+-----+---+---------+--------+--------------------+----------+\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n |-- source: string (nullable = true)\n |-- date: integer (nullable = false)\n |-- message: string (nullable = true)\n |-- category: string (nullable = true)\n\n+-----+\n| name|\n+-----+\n|  Bas|\n| Test|\n| John|\n|Maria|\n+-----+\n\ndataframe1\nall: org.apache.spark.sql.DataFrame = [name: string, age: int, source: string, date: int, message: string, category: string]\n[Bas,37,Twitter,20160414,Preparing for the IoT Tech Day 2016!,Technology]\n[Test,12,Facebook,20140511,I really like my dog.,Pets]\n[John,18,Pinterest,20150904,New Fifa is really awesome,Games ]\n[Maria,21,Twitter,20160118,Going shopping to get new sneakers!,Shoes]\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"lineNumbers":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460584611627_-1778772518","id":"20160413-235651_1297268440","dateCreated":"Apr 13, 2016 11:56:51 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1178","text":"import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\r\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\r\n\r\nprintln(data.count) + \" records found.\"\r\n\r\n// create vectors\r\nvar vectors = data.map(x => Vectors.dense(x.name.hashCode, x.age, x.source.hashCode, x.date, x.message.hashCode, x.category.hashCode))\r\n\r\nval summary = Statistics.colStats(vectors)\r\n\r\nprintln(\"mean values: \" + summary.mean)\r\nprintln(\"variances: \" + summary.variance)","dateUpdated":"Apr 14, 2016 12:30:20 AM","dateFinished":"Apr 14, 2016 12:30:23 AM","dateStarted":"Apr 14, 2016 12:30:20 AM","title":"3. Show Simple Statistics","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n4\nres97: String = () records found.\nvectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[51] at map at <console>:50\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@339c17c5\nmean values: [1.977450575E7,22.0,1.0153304975E9,2.015298675E7,1.6050267675E8,6.3052063375E8]\nvariances: [1.313620575068263E15,114.0,4.4122540741275053E17,8.866698491666667E7,2.2894079974325696E18,1.01100457526080602E18]\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"lineNumbers":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460584623587_-843802043","id":"20160413-235703_1464691257","dateCreated":"Apr 13, 2016 11:57:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1195","text":"var corr = Statistics.corr(vectors, \"pearson\")\r\n\r\nprintln(\"Correlations:\")\r\nprintln(corr.toString(10, 1000))\r\n","dateUpdated":"Apr 14, 2016 12:30:26 AM","dateFinished":"Apr 14, 2016 12:30:28 AM","dateStarted":"Apr 14, 2016 12:30:26 AM","title":"4. Correlations","result":{"code":"SUCCESS","type":"TEXT","msg":"corr: org.apache.spark.mllib.linalg.Matrix = \n1.0                   -0.0933776305860519   ... (6 total)\n-0.0933776305860519   1.0                   ...\n-0.25895149577497717  -0.14819881118989112  ...\n0.48026534061473686   0.7869802989664519    ...\n-0.7780329731431388   0.5734336594399045    ...\n-0.3567878496007655   -0.12093687442490747  ...\nCorrelations:\n1.0                   -0.0933776305860519   -0.25895149577497717   0.48026534061473686    -0.7780329731431388   -0.3567878496007655   \n-0.0933776305860519   1.0                   -0.14819881118989112   0.7869802989664519     0.5734336594399045    -0.12093687442490747  \n-0.25895149577497717  -0.14819881118989112  1.0                    -0.015238401115367678  -0.2432817483058034   0.9946278664717012    \n0.48026534061473686   0.7869802989664519    -0.015238401115367678  1.0                    -0.05395661813576369  -0.0547595976260914   \n-0.7780329731431388   0.5734336594399045    -0.2432817483058034    -0.05395661813576369   1.0                   -0.14709003476958804  \n-0.3567878496007655   -0.12093687442490747  0.9946278664717012     -0.0547595976260914    -0.14709003476958804  1.0                   \n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"lineNumbers":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460584638483_1731737743","id":"20160413-235718_1781324116","dateCreated":"Apr 13, 2016 11:57:18 PM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1212","text":"import org.apache.spark.mllib.regression.LabeledPoint\r\nimport org.apache.spark.mllib.regression.LinearRegressionModel\r\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\r\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\r\nimport org.apache.spark.mllib.linalg.Vectors\r\n\r\n// nonlinear regression is not part of Spark yet...\r\n// split data into two parts: before and after 50. (u-shape of data)\r\n// linear regression with stochastic gradient descent needs scaled data; divide the happiness scores by 10 to put them in a 0..1 range\r\n\r\n// LabeledPoint: class that represents the features and labels of a data point. LabeledPoint(double label, Vector features)\r\nval points = data.map(line => LabeledPoint(line.category.hashCode, Vectors.dense(line.message.hashCode, line.age))).collect()\r\nval rdd = sc.makeRDD(points)\r\n\r\nval numIterations = 100\r\n// step size is learning rate.\r\nval stepSize = 0.5\r\n\r\n// train\r\nval algorithm = new LinearRegressionWithSGD()   // LinearRegressionModel\r\nalgorithm.setIntercept(true).setValidateData(false)\r\nalgorithm.optimizer.setNumIterations(numIterations).setStepSize(stepSize)\r\nval model = algorithm.run(rdd)\r\n\r\nmodel.save(sc, \"/home/scala-academy/social_media.model\")\r\n\r\nprintln(model.intercept)\r\nprintln(model.weights)","dateUpdated":"Apr 14, 2016 12:33:14 AM","dateFinished":"Apr 14, 2016 12:33:32 AM","dateStarted":"Apr 14, 2016 12:33:14 AM","title":"5. Create Regression Model","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\nimport org.apache.spark.mllib.linalg.Vectors\npoints: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((3.14138924E8,[2.019885732E9,37.0]), (2484052.0,[5.53494747E8,12.0]), (2.125599135E9,[-3.65735294E8,18.0]), (7.9860424E7,[-1.565634478E9,21.0]))\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = ParallelCollectionRDD[61] at makeRDD at <console>:74\nnumIterations: Int = 100\nstepSize: Double = 0.5\nalgorithm: org.apache.spark.mllib.regression.LinearRegressionWithSGD = org.apache.spark.mllib.regression.LinearRegressionWithSGD@70456757\nres147: algorithm.type = org.apache.spark.mllib.regression.LinearRegressionWithSGD@70456757\nres148: algorithm.optimizer.type = org.apache.spark.mllib.optimization.GradientDescent@9e2070c\nmodel: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = NaN, numFeatures = 2\nNaN\n[NaN,NaN]\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","lineNumbers":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460584656499_-2014730863","id":"20160413-235736_479475408","dateCreated":"Apr 13, 2016 11:57:36 PM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1229","text":"// Evaluate young and old models on training examples and compute training error\r\nval valuesAndPreds_young = data_young.map { point =>\r\n  val prediction = model_young.predict(point.features)\r\n  println(\"Prediction YOUNG: point=\" + point.label + \", features=\" + point.features + \", prediction=\" + prediction)\r\n  (point.label, prediction)\r\n}\r\nval MSE_young = valuesAndPreds_young.map{case(v, p) => math.pow((v - p), 2)}\r\n\r\nval valuesAndPreds_old = data_old.map { point =>\r\n  val prediction = model_old.predict(point.features)\r\n  println(\"Prediction OLD: point=\" + point.label + \", features=\" + point.features + \", prediction=\" + prediction)\r\n  (point.label, prediction)\r\n}\r\nval MSE_old = valuesAndPreds_old.map{case(v, p) => math.pow((v - p), 2)}\r\n\r\n// MSE incidates the goodness of fit; should be smaller than 1\r\nprintln(\"Young training Mean Squared Error = \" + (MSE_young.sum / MSE_young.size))\r\nprintln(\"Old training Mean Squared Error =   \" + (MSE_old.sum / MSE_old.size))","dateUpdated":"Apr 13, 2016 11:58:44 PM","title":"6. Evaluate model"}],"name":"Fast Data","id":"2BJZB8R4U","angularObjects":{"2BJ3TW16G":[],"2BF63DQHX":[],"2BFY1EPFB":[],"2BJ9M1CC9":[],"2BG5UWC8Y":[],"2BHR6JRFA":[],"2BH6UFQQZ":[],"2BHEPFEUR":[],"2BF1QBF13":[],"2BHAYFSCW":[],"2BJSPVTQ4":[],"2BJ1Q79PF":[],"2BHA6B6RC":[],"2BFYSXAXC":[]},"config":{"looknfeel":"default"},"info":{}}